## Сравнение производительности от 2 PostgreSQL кластеров на большом объеме данных»

Сравнение производительности в рамках проекта будет производиться на примере отказоустойчивого кластера Patroni и кластера, созданного при помощи pg_auto_failover.

Для начала развернём кластер Patroni на 3-х нодах: 
patroni-1 192.168.145.141 - PostgreSQL Master, etcd
patroni-2 192.168.145.136 - PostgreSQL Replica, etcd
patroni-3 192.168.145.138 - etcd.

1. Установка etcd на 3 ВМ:
```
sudo apt update && sudo apt upgrade -y && sudo apt install -y etcd
```
2. Проверим состояние etcd:
```
hostname; ps -aef | grep etcd | grep -v grep
```

Остановим etcd:
```
sudo systemctl stop etcd
```

4. Создаём файл конфигурации:
```
sudo nano /etc/default/etcd
```
```
ETCD_NAME="$(hostname)"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"
ETCD_ADVERTISE_CLIENT_URLS="http://$(hostname):2379"
ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://$(hostname):2380"
ETCD_INITIAL_CLUSTER_TOKEN="PatroniCluster"
ETCD_INITIAL_CLUSTER="etcd1=http://etcd1:2380,etcd2=http://etcd2:2380,etcd3=http://etcd3:2380"
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_DATA_DIR="/var/lib/etcd"
ETCD_ENABLE_V2="true"
```

5. Запускаем etcd:
```
sudo systemctl start etcd
```

6. Проверяем автозагрузку:
```
systemctl is-enabled etcd
```

7. Проверяем состояние кластера:
```
etcdctl --cluster=true endpoint health
http://etcd1:2379 is healthy: successfully committed proposal: took = 9.229656ms
http://etcd3:2379 is healthy: successfully committed proposal: took = 9.012942ms
http://etcd2:2379 is healthy: successfully committed proposal: took = 9.292935ms
```

8. Устанавливаем PostgreSQL на patroni-1 и patroni-2:
```
apt install postgresql
```

9. Добавляем пользователя replicator:
```
sudo -u postgres psql
create user replicator replication login encrypted password 'passwdord';
```

10. Задаём пароль для пользователя postgres:
```
\password postgres;
```

11. Подключаем расширения:
```
CREATE EXTENSION pg_stat_statements;
LOAD 'auto_explain';
```

12. Создаём пользователя pgbouncer:
```
create user pgbouncer password 'password';
\q
```

13. Редактируем pg_hba.conf:
```
sudo nano /etc/postgresql/15/main/pg_hba.conf
# Database administrative login by Unix domain socket
local   all             postgres                                peer

# TYPE  DATABASE        USER            ADDRESS                 METHOD

# "local" is for Unix domain socket connections only
local   all             all                                     peer
# IPv4 local connections:
host    all             all             127.0.0.1/32            md5
host    all             all             10.0.2.0/24             md5
# IPv6 local connections:
host    all             all             ::1/128                 md5
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   replication     all                                     peer
host    replication     replicator      localhost               trust
host    replication     all             0.0.0.0/0               trust
host    replication     replicator      0.0.0.0/0               trust
```

14. На ноде patroni-2 удаляем содержимое каталога /var/lib/postgresql/15/main:
```
sudo rm -rf /var/lib/postgresql/15/main/*
```

15. Установка Keepalived на две ноды:
```
sudo apt install keepalived
```

16. Правим конфигурацию :
```
sudo nano /etc/keepalived/keepalived.conf
```

На patroni-1:

```
$ sudo nano /etc/keepalived/keepalived.conf
global_defs {
   router_id ocp_vrrp
   enable_script_security
   script_user root
   dynamic_interfaces
}
 
vrrp_script haproxy_check {
   script "/usr/libexec/keepalived/haproxy_check.sh"
   interval 5 # check every 5 seconds
   weight 2 # add 2 points of prio if OK
}
 
vrrp_instance VI_1 {
   interface enp0s3
   virtual_router_id 11
   priority  101 # 101 on master, 100 on backup
   advert_int 10
   state  MASTER
   virtual_ipaddress {
       10.0.2.11
   }
   track_script {
       haproxy_check
   }
   authentication {
      auth_type PASS
      auth_pass ehr0wg1chww8
   }
}
```

На patroni-2:

```
global_defs {
   router_id ocp_vrrp
   enable_script_security
   script_user root
   dynamic_interfaces
}
 
vrrp_script haproxy_check {
   script "/usr/libexec/keepalived/haproxy_check.sh"
   interval 5 # check every 5 seconds
   weight 2 # add 2 points of prio if OK
}
 
vrrp_instance VI_1 {
   interface enp0s3
   virtual_router_id 11
   priority  100 # 101 on master, 100 on backup
   advert_int 10
   state  BACKUP
   virtual_ipaddress {
       10.0.2.11
   }
   track_script {
       haproxy_check
   }
   authentication {
      auth_type PASS
      auth_pass ehr0wg1chww8
   }
}
```

17. Создадим каталог и скрипт проверки HAproxy:
```
sudo mkdir -p /usr/libexec/keepalived/
sudo nano /usr/libexec/keepalived/haproxy_check.sh
```
```
#!/bin/bash
/bin/kill -0 `cat /var/run/haproxy/haproxy.pid`
```

18. Назначаем права:
```
sudo chmod 700 /usr/libexec/keepalived/haproxy_check.sh
sudo chmod +x /usr/libexec/keepalived/haproxy_check.sh
```
19. Запускаем сервис, добавляем в автозагрузку, проверяем:
```
sudo systemctl start keepalived
sudo systemctl enable keepalived
sudo systemctl status keepalived
```
20. Установка Patroni на две ноды:
```
sudo apt -y install python3 python3-pip python3-dev python3-psycopg2 libpq-dev
sudo pip3 install --upgrade pip --break-system-packages 
sudo pip3 install psycopg2 --break-system-packages 
sudo pip3 install psycopg2-binary --break-system-packages 
sudo pip3 install wheel --break-system-packages 
sudo pip3 install patroni --break-system-packages 
sudo pip3 install python-etcd --break-system-packages 
apt install patroni
```

21. Создадим конфиг patroni.yml:

```
$ sudo nano /etc/patroni/patroni.yml
---

scope: postgres-cluster # одинаковое значение на всех узлах
name: patroni-1 # разное значение на всех узлах
namespace: /service/ # одинаковое значение на всех узлах

restapi:
  listen: 192.168.145.141:8008 # разное значение на всех узлах
  connect_address: 192.168.145.141:8008 # разное значение на всех узлах
  authentication:
    username: patroni
    password: 'passwdord'

etcd:
  hosts: 192.168.145.141:2379,192.168.145.136:2379,192.168.145.138:2379 # список всех узлов, на которых установлен etcd

bootstrap:
  method: initdb
  dcs:
    ttl: 60
    loop_wait: 10
    retry_timeout: 27
    maximum_lag_on_failover: 2048576
    master_start_timeout: 300
    synchronous_mode: true
    synchronous_mode_strict: false
    synchronous_node_count: 1
    # standby_cluster:
      # host: 127.0.0.1
      # port: 1111
      # primary_slot_name: patroni
    postgresql:
      use_pg_rewind: false
      use_slots: true
      parameters:
        max_connections: 800
        superuser_reserved_connections: 5
        max_locks_per_transaction: 64
        max_prepared_transactions: 0
        huge_pages: try
        shared_buffers: 512MB
        work_mem: 128MB
        maintenance_work_mem: 256MB
        effective_cache_size: 4GB
        checkpoint_timeout: 15min
        checkpoint_completion_target: 0.9
        min_wal_size: 2GB
        max_wal_size: 4GB
        wal_buffers: 32MB
        default_statistics_target: 1000
        seq_page_cost: 1
        random_page_cost: 4
        effective_io_concurrency: 2
        synchronous_commit: on
        autovacuum: on
        autovacuum_max_workers: 5
        autovacuum_vacuum_scale_factor: 0.01
        autovacuum_analyze_scale_factor: 0.02
        autovacuum_vacuum_cost_limit: 200
        autovacuum_vacuum_cost_delay: 20
        autovacuum_naptime: 1s
        max_files_per_process: 4096
        archive_mode: on
        archive_timeout: 1800s
        archive_command: cd .
        wal_level: replica
        wal_keep_segments: 130
        max_wal_senders: 10
        max_replication_slots: 10
        hot_standby: on
        hot_standby_feedback: True
        wal_log_hints: on
        shared_preload_libraries: pg_stat_statements,auto_explain
        pg_stat_statements.max: 10000
        pg_stat_statements.track: all
        pg_stat_statements.save: off
        auto_explain.log_min_duration: 10s
        auto_explain.log_analyze: true
        auto_explain.log_buffers: true
        auto_explain.log_timing: false
        auto_explain.log_triggers: true
        auto_explain.log_verbose: true
        auto_explain.log_nested_statements: true
        track_io_timing: on
        log_lock_waits: on
        log_temp_files: 0
        track_activities: on
        track_counts: on
        track_functions: all
        log_checkpoints: on
        logging_collector: on
        log_statement: mod
        log_truncate_on_rotation: on
        log_rotation_age: 1d
        log_rotation_size: 0
        log_line_prefix: '%m [%p] %q%u@%d '
        log_filename: 'postgresql-%a.log'
        log_directory: /var/log/postgresql

  initdb:  # List options to be passed on to initdb
    - encoding: UTF8
    - locale: en_US.UTF-8
    - data-checksums

  pg_hba:  # должен содержать адреса ВСЕХ машин, используемых в кластере
    - host all all 0.0.0.0/0 md5
    - host replication replicator 0.0.0.0/0 trust

postgresql:
  listen: 192.168.145.141,127.0.0.1:5432 # разное значение на всех узлах
  connect_address: 192.168.145.141:5432 # разное значение на всех узлах
  use_unix_socket: true
  data_dir: /var/lib/postgresql/14/main
  bin_dir: /usr/lib/postgresql/14/bin
  config_dir: /etc/postgresql/14/main
  pgpass: /var/lib/postgresql/.pgpass_patroni
  authentication:
    replication:
      username: replicator
      password: passwd
    superuser:
      username: postgres
      password: passwd
  parameters:
    unix_socket_directories: /var/run/postgresql
    stats_temp_directory: /var/lib/pgsql_stats_tmp

  remove_data_directory_on_rewind_failure: false
  remove_data_directory_on_diverged_timelines: false

#  callbacks:
#    on_start:
#    on_stop:
#    on_restart:
#    on_reload:
#    on_role_change:

  create_replica_methods:
    - basebackup
  basebackup:
    max-rate: '100M'
    checkpoint: 'fast'

watchdog:
  mode: off  # Allowed values: off, automatic, required
  device: /dev/watchdog
  safety_margin: 5

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false

  # specify a node to replicate from (cascading replication)
#  replicatefrom: (node name)
```

22. Назначаем права:
```
sudo chown postgres:postgres -R /etc/patroni
sudo chown postgres:postgres -R /etc/patroni
sudo chmod 700 /etc/patroni
```

23. Создаем каталог, назначаем владельца:
```
sudo chown postgres:postgres /var/lib/pgsql_stats_tmp
sudo mkdir /var/lib/pgsql_stats_tmp
sudo chown postgres:postgres /var/lib/pgsql_stats_tmp
```

24. Тестируем старт (сразу на двух нодах):
```
sudo -u postgres patroni /etc/patroni/patroni.yml
sudo -u postgres patroni /etc/patroni/patroni.yml
```
25. Должен создастся файл .pgpass_patroni, назначаем владельца, права:
```
sudo chown postgres:postgres /var/lib/postgresql/.pgpass_patroni
sudo chmod 0600 /var/lib/postgresql/.pgpass_patroni
sudo chown postgres:postgres /var/lib/postgresql/.pgpass_patroni
sudo chmod 0600 /var/lib/postgresql/.pgpass_patroni
```

26. Создаем Sustemd Unit (на каждой ноде):
```
[Unit]
Description=High availability PostgreSQL Cluster
After=syslog.target network.target

[Service]
Type=simple
User=postgres
Group=postgres

# Read in configuration file if it exists, otherwise proceed
EnvironmentFile=-/etc/patroni_env.conf

# Start the patroni process
ExecStart=/usr/bin/patroni /etc/patroni/patroni.yml

# Send HUP to reload from patroni.yml
ExecReload=/bin/kill -s HUP $MAINPID

# only kill the patroni process, not it's children, so it will gracefully stop postgres
KillMode=process

# Give a reasonable amount of time for the server to start up/shut down
TimeoutSec=60

# Do not restart the service if it crashes, we want to manually inspect database on failure
Restart=no

[Install]
WantedBy=multi-user.target
```
27. Добавляем сервис а автозагрузку, стартуем, смотрим статус:
```
sudo systemctl daemon-reload
sudo systemctl start patroni
sudo systemctl status patroni
sudo systemctl enable patroni
```

Кластер успешно запущен.

28. Смотрим состояние кластера:
```
sudo patronictl -c /etc/patroni/patroni.yml list
```
Вывод:

```
| Member    | Host            | Role         | State   | TL | Lag in MB |
+-----------+-----------------+--------------+---------+----+-----------+
| patroni-1 | 192.168.145.141 | Leader       | running |  5 |           |
| patroni-2 | 192.168.145.136 | Sync Standby | running |  5 |         0 |
+-----------+-----------------+--------------+---------+----+-----------+
```

Нода patroni-1 является ведущей, нода patroni-2 работает в режиме синхронизацией с patroni-1.

29. Установим haproxy на две ноды:
```
sudo apt -y install haproxy
```

30. Настроим:
```
sudo nano /etc/haproxy/haproxy.cfg
```

На patroni-1:
```
global
    maxconn 100000
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    mode               tcp
    log                global
    retries            2
    timeout queue      5s
    timeout connect    5s
    timeout client     60m
    timeout server     60m
    timeout check      15s

listen stats
    mode http
    bind 192.168.145.141:7000
    stats enable
    stats uri /

listen postgres_master
    bind *:5000
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /master
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008

listen postgres_replicas
    bind *:5001
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /replica
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008

listen postgres_replicas_sync
    bind *:5002
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /sync
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008

listen postgres_replicas_async
    bind *:5003
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /async
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008
```

На patroni-2:

```
global
    maxconn 100000
    log /dev/log    local0
    log /dev/log    local1 notice
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
    stats timeout 30s
    user haproxy
    group haproxy
    daemon

defaults
    mode               tcp
    log                global
    retries            2
    timeout queue      5s
    timeout connect    5s
    timeout client     60m
    timeout server     60m
    timeout check      15s

listen stats
    mode http
    bind 192.168.145.136:7000
    stats enable
    stats uri /

listen postgres_master
    bind *:5000
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /master
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 4 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008

listen postgres_replicas
    bind *:5001
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /replica
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008

listen postgres_replicas_sync
    bind *:5002
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /sync
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008
    
listen postgres_replicas_async
    bind *:5003
    maxconn 10000
    option tcplog
    option httpchk OPTIONS /async
    balance roundrobin
    http-check expect status 200
    default-server inter 3s fastinter 1s fall 3 rise 2 on-marked-down shutdown-sessions
    server patroni-1 192.168.145.141:6432 check port 8008
    server patroni-2 192.168.145.136:6432 check port 8008
```

31. Проверяем оба конфига:
```
sudo haproxy -f /etc/haproxy/haproxy.cfg -c
```

Конфиг работоспособен:
```
Configuration file is valid
```

32. Подключимся к psql и создадим базу для чикагского такси:
```
sudo -u postgres psql
create database taxi;
\c taxi;
```

33. Создадим таблицу для загрузки данных:
```
create table taxi_trips (
unique_key text, 
taxi_id text, 
trip_start_timestamp TIMESTAMP, 
trip_end_timestamp TIMESTAMP, 
trip_seconds bigint, 
trip_miles numeric, 
pickup_census_tract bigint, 
dropoff_census_tract bigint, 
pickup_community_area bigint, 
dropoff_community_area bigint, 
fare numeric, 
tips numeric, 
tolls numeric, 
extras numeric, 
trip_total numeric, 
payment_type text, 
company text, 
pickup_latitude numeric, 
pickup_longitude numeric, 
pickup_location text, 
dropoff_latitude numeric, 
dropoff_longitude numeric, 
dropoff_location text
);
```
34. Проверик со второй ноды, что таблица реплицировалась на неё:
```
   Name    |  Owner   | Encoding |   Collate   |    Ctype    | ICU Locale | Locale Provider |   Access privileges
-----------+----------+----------+-------------+-------------+------------+-----------------+-----------------------
 postgres  | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |            | libc            |
 taxi      | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |            | libc            |
 template0 | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
 template1 | postgres | UTF8     | ru_RU.UTF-8 | ru_RU.UTF-8 |            | libc            | =c/postgres          +
           |          |          |             |             |            |                 | postgres=CTc/postgres
(4 rows)
```

35. Создадим и запустим скрипт для загрузки данных:
```
#!/bin/bash

for f in /tmp/taxi/taxi*
do
        echo -e "Processing $f file..."
        psql "host=localhost port=5432 dbname=taxi user=postgres " -c "\\COPY taxi_trips FROM PROGRAM 'cat $f' CSV HEADER"
done
```

36. Установим pg_bouncer:
```
sudo apt -y install pgbouncer
sudo mv /etc/pgbouncer/pgbouncer.ini /etc/pgbouncer/pgbouncer.ini.origin
sudo nano /etc/pgbouncer/pgbouncer.ini

[databases]
postgres = host=127.0.0.1 port=5432 dbname=postgres
* = host=127.0.0.1 port=5432

[pgbouncer]
logfile = /var/log/postgresql/pgbouncer.log
pidfile = /var/run/postgresql/pgbouncer.pid
listen_addr = *
listen_port = 6432
unix_socket_dir = /var/run/postgresql
auth_type = md5
#auth_type = trust
auth_file = /etc/pgbouncer/userlist.txt
auth_user = postgres
auth_query = SELECT usename, password FROM pg_shadow WHERE usename=$1
#admin_users = pgbouncer, postgres
admin_users = postgres
ignore_startup_parameters = extra_float_digits,geqo,search_path

pool_mode = session
#pool_mode = transaction
server_reset_query = DISCARD ALL
max_client_conn = 10000
#default_pool_size = 20
reserve_pool_size = 1
reserve_pool_timeout = 1
max_db_connections = 1000
#max_client_conn = 900
default_pool_size = 500
pkt_buf = 8192
listen_backlog = 4096
log_connections = 1
log_disconnections = 1

```

37. Создадим файл userlist.txt:

sudo nano /etc/pgbouncer/userlist.txt
"postgres" "passwd"
"pgbouncer" "passwd"
sudo nano /etc/pgbouncer/userlist.txt
"postgres" "passwd"
"pgbouncer" "passwd"

38. Перезапускаем PGbouncer на нодах
```
sudo systemctl restart pgbouncer
sudo systemctl restart pgbouncer
```

39. Подключимся к postgresql и включим timing:
```
\timing
```

40. Выполним запрос:
```
\c taxi;

SELECT payment_type, 
    round(sum(tips)/sum(trip_total)*100, 0) + 0 as tips_percent, 
    count(*) as c
FROM taxi_trips
group by payment_type
order by 3;
```

Время выполнения запроса составило 6 минут 35 секунд.

## pg_auto_failover

Затем развернём кластер из двух нод PostgreSQL и одной ноды управления pg_auto_failover:

1. Создать 3 ВМ: 2 для PostgreSQL, 1 для монитора. Удалить на каждой автоматически созданный кластер и установить auto-failover:
```
sudo pg_dropcluster 15 main --stop
```
```
apt install postgresql-15-auto-failover -y 
```
2. Настройка pgmon:
```
sudo su - postgres
mkdir -p /var/lib/postgresql/15/pgmon
export PATH=/usr/lib/postgresql/15/bin/:$PATH
export PGDATA=/var/lib/postgresql/15/pgmon

nano /var/lib/postgresql/15/pgmon/pg_hba.conf # прописать доступ
```
3. Выполнить инициализацию:
```

pg_autoctl create monitor \
    --auth trust \
    --no-ssl \
    --pgport 5432 \
    --hostname pg-mon
    
exit    
```
4. Запустить сервис:
```
sudo -i
pg_autoctl -q show systemd --pgdata "/var/lib/postgresql/15/pgmon" | tee /etc/systemd/system/pgautofailover.service
systemctl daemon-reload
systemctl enable --now pgautofailover.service
exit
```

5. Настройка ВМ с БД:
```
sudo su - postgres
mkdir -p /var/lib/postgresql/15/pgmon
export PATH=/usr/lib/postgresql/15/bin/:$PATH
export PGDATA=/var/lib/postgresql/15/pgmon
```

6. Создать мастер-ноду:
```
pg_autoctl create postgres     --auth trust     --no-ssl  --pgdata /var/lib/postgresql/15/data   --monitor postgres://autoctl_node@192.168.145.144:5432/pg_auto_failover?sslmode=prefer     --run

  ```
  
 7. Аналогично создать реплику на второй ВМ. Проверить, что всё работает на мониторе:
 ```
export PATH=/usr/lib/postgresql/15/bin/:$PATH
pg

pg_autoctl show state


  Name |  Node |  Host:Port |       TLI: LSN |   Connection |      Reported State |      Assigned State
-------+-------+------------+----------------+--------------+---------------------+--------------------
node_1 |     1 |  pg-1:5432 |   1: 0/3000148 |   read-write |             primary |             primary
node_2 |     2 |  pg-2:5432 |   1: 0/3000148 |    read-only |           secondary |           secondary
```

Кластер успешно функционирует, нода pg-1 в роли master.
Посмотреть ссылки для подключения:

```
pg_autoctl show uri
        Type |    Name | Connection String
-------------+---------+-------------------------------
     monitor | monitor | postgres://autoctl_node@pg-mon:5432/pg_auto_failover?sslmode=prefer
   formation | default | postgres://pg-2:5432,pg-1:5432/postgres?target_session_attrs=read-write&sslmode=prefer
```

8. Проверим подключение:
```
psql postgres://pg-1:5432,pg-2:5432
```

9. Создадим базу данных:
```
create database taxi;
\c taxi;
```
10. Создадим таблицу для загрузки данных:
```
create table taxi_trips (
unique_key text, 
taxi_id text, 
trip_start_timestamp TIMESTAMP, 
trip_end_timestamp TIMESTAMP, 
trip_seconds bigint, 
trip_miles numeric, 
pickup_census_tract bigint, 
dropoff_census_tract bigint, 
pickup_community_area bigint, 
dropoff_community_area bigint, 
fare numeric, 
tips numeric, 
tolls numeric, 
extras numeric, 
trip_total numeric, 
payment_type text, 
company text, 
pickup_latitude numeric, 
pickup_longitude numeric, 
pickup_location text, 
dropoff_latitude numeric, 
dropoff_longitude numeric, 
dropoff_location text
);
```

11. Создадим и запустим bash-скрипт для загрузки данных в Postgresql:
```

#!/bin/bash

for f in /tmp/taxi/taxi*
do
        echo -e "Processing $f file..."
        psql postgres://pg-2:5432,pg-1:5432/taxi?target_session_attrs=read-write  -c "\\COPY taxi_trips FROM PROGRAM 'cat $f' CSV HEADER"
done

```

12. Проверим, что данные успешно загрузились:
```
taxi=# \l+ taxi
```

13. Включить секундомер:
```
\timing
```

14. Выполним запрос:
```
SELECT payment_type, 
    round(sum(tips)/sum(trip_total)*100, 0) + 0 as tips_percent, 
    count(*) as c
FROM taxi_trips
group by payment_type
order by 3;
```
Время выполнения запроса составило 5 минут 55 секунд. 

## Заключение
  Время выполнения одиночного сложного запроса примерно одинаково для кластеров Patroni и pg_auto_failover. 
  Оба кластера были развёрнуты на серверах с одинаковыми характеристиками. Кластер Patroni развёрнут без балансировки нагрузки запросов на чтение между серверами. В кластере pg_auto_failover балансировка нагрузки стандартными средствами не поддерживается.   
  Кластер на основе pg_autofailover более прост в настройке и обслуживании, однако имеет меньше гибких настроек. При необходимости повышения производительности кластера понадобится использовать дополнительные решения для балансировки нагрузки.
  Для приложений, осуществляющих множество запросов к базе одновременно, кластер Patroni более предпочтителен, поскольку включает возможность балансировки нагрузки. 
  Кластер на основе pg_auto_failover лучше подойдёт для более простых решений по кластеризации. 
